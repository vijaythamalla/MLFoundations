{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassfier,VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,precision_recall_curve\n",
    "from sklearn.metrics import roc_curve,roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone,BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* precision = TP/(TP+FP)\n",
    "* recall = TP/(TP+FN)\n",
    "     * TP - TRUE POSITIVES (model predicting actual class as correcct class)\n",
    "     * FP - FALSE POSITIVES (model predicting wrong class as correct class)\n",
    "     * TN - TRUE NEGATIVES (model predicting wrong class as wrong class)\n",
    "     * FN - FALSE NEGATIVES (model predicting actual class as wrong class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is the harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly watching the predictions you can get the decision scores of model by using decision_function() method instead of predict() method. So that you can try out different thresholds as per your needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you decide which threshold to use? First, use the cross_val_predict() function to get the scores of all instances in the training set, but this time specify that you want to return decision scores instead of predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,method=\"decision_function\")**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these scores, use the precision_recall_curve() function to compute precision and recall for all possible thresholds:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use Matplotlib to plot precision and recall as functions of the threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "#     plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "#     plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "#     plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n",
    "#     plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n",
    "#     plt.grid(True)                              # Not shown\n",
    "#     plt.axis([-50000, 50000, 0, 1])             # Not shown\n",
    "\n",
    "\n",
    "\n",
    "# recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\n",
    "# threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(8, 4))                                                                  # Not shown\n",
    "# plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "# plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 # Not shown\n",
    "# plt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")                                # Not shown\n",
    "# plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")# Not shown\n",
    "# plt.plot([threshold_90_precision], [0.9], \"ro\")                                             # Not shown\n",
    "# plt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                             # Not shown\n",
    "# save_fig(\"precision_recall_vs_threshold_plot\")                                              # Not shown\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/PRvsT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to select a good precision/recall trade-off is to plot precision directly against recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_precision_vs_recall(precisions, recalls):\n",
    "#     plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "#     plt.xlabel(\"Recall\", fontsize=16)\n",
    "#     plt.ylabel(\"Precision\", fontsize=16)\n",
    "#     plt.axis([0, 1, 0, 1])\n",
    "#     plt.grid(True)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plot_precision_vs_recall(precisions, recalls)\n",
    "# plt.plot([0.4368, 0.4368], [0., 0.9], \"r:\")\n",
    "# plt.plot([0.0, 0.4368], [0.9, 0.9], \"r:\")\n",
    "# plt.plot([0.4368], [0.9], \"ro\")\n",
    "# save_fig(\"precision_vs_recall_plot\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/PvsR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The *receiver operating characteristic* (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the *true positive rate* (another name for recall) against the *false positive rate* (FPR). The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to 1 – the *true negative rate* (TNR), which is the ratio of negative instances that are correctly classified as negative. The TNR is also called *specificity*. Hence, the ROC curve plots *sensitivity* (recall) versus 1 – *specificity*\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_roc_curve(fpr, tpr, label=None):\n",
    "#     plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "#     plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "#     plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n",
    "#     plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n",
    "#     plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n",
    "#     plt.grid(True)                                            # Not shown\n",
    "\n",
    "# plt.figure(figsize=(8, 6))                         # Not shown\n",
    "# plot_roc_curve(fpr, tpr)\n",
    "# plt.plot([4.837e-3, 4.837e-3], [0., 0.4368], \"r:\") # Not shown\n",
    "# plt.plot([0.0, 4.837e-3], [0.4368, 0.4368], \"r:\")  # Not shown\n",
    "# plt.plot([4.837e-3], [0.4368], \"ro\")               # Not shown\n",
    "# save_fig(\"roc_curve_plot\")                         # Not shown\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ROC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_pred = cross_val_predict(clf, X_train, y_train, cv=3)\n",
    "# conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "# plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s focus the plot on the errors. First, you need to divide each value in the confusion matrix by the number of images in the corresponding class so that you can compare error rates instead of absolute numbers of errors (which would make abundant classes look unfairly bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "# norm_conf_mx = conf_mx / row_sums\n",
    "# np.fill_diagonal(norm_conf_mx, 0)  # Fill the diagonal with zeros to keep only the errors\n",
    "# plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
